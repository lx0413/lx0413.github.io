---
title: ""
author: ""
date: ""
output:
  bookdown::pdf_document2: 
    toc: false
    toc_depth: 2
    fig_caption: yes
    number_sections: yes
bibliography: 
  - Ch3_library.bib
csl: academy-of-management-review.csl
urlcolor: black
linkcolor: black
fontsize: 10pt
linestretch: 2
geometry: "left=1.25in, right=1.25in, top=1in, bottom=1in"
indent: true
header-includes:
- \usepackage{indentfirst}
---


\begin{centering}

\vspace{2 cm}

\Huge

{\bf Nowcasting Business Cycle Phases with High and Mixed Frequency Data}



\vspace{2 cm}

\Large
by

Xiang LI

\vspace{2 cm}

\normalsize

\end{centering}




**Abstract**: In this paper I investigate whether the use of higher frequency data can improve the speed at which business cycle peaks and troughs can be identified in U.S. data over the existing literature that focuses primarily on monthly and quarterly data. This paper is the first to systematically evaluate the ability of high frequency data to improve upon the timeliness with which new expansions and recessions can be identified. The use of higher frequency data significantly and consistenly improves the speed at which expansions and recessions can be identified in the U.S. since 1980. 

\newpage


# Introduction

A common definition of the business cycle is of alternating periods of expansion and recession. However, these regimes are not explicitly observed, and their timing is instead commonly estimated from time-series data. For the United States, the National Bureau of Economic Research (NBER) maintains a chronology of alternating dates of peaks and troughs of the U.S. business cycle. However, the NBER’s identification of these turning points generally occurs long after the fact. For example, the December 2007 peak of the Great Recession was not announced by the NBER until December 1, 2008. Given the substantial costs on individuals and firms, it would be useful to have more timely identification of turning points for policymakers, firms, consumers, and financial markets participants.

Given this need, an active literature has worked to develop statistical techniques to “nowcast” business cycle turning points toward the end of the observable sample period. Nearly all of this literature uses relatively low frequency data to nowcast turning points, namely monthly or quarterly data. It seems reasonable to expect that significant gains in the speed with which turning points dates can be identified might be achieved by also incorporating higher frequency data at the weekly or daily frequency. There are several reasons for this expectation. First, higher frequency data would allow additional variables to be incorporated than what has typically been used in the business cycle nowcasting literature, such as financial variables or initial claims of unemployment insurance. Second, the use of higher frequency data would allow for more frequent updating of the model, since most monthly or quarterly variables relevant for tracking the business cycle are released in a cluster around the end of the month. Third, higher frequency variables generally have much shorter reporting lags. For example, initial claims on unemployment insurance for a given week are available only a few days after the week ends, whereas many monthly series, such as personal income, are released only after a full month delay.

In this paper I investigate whether the use of higher frequency data can improve the speed at which business cycle peaks and troughs can be identified in U.S. data over the existing literature that focuses primarily on monthly and quarterly data. To identify turning points I take a three-step approach. First, I use the mixed frequency dynamic factor model of @Aruoba_Diebold_Scotti_2009 (ADS) to extract a coincident index of real economic activity using weekly, monthly and quarterly data. Second, I use a supervised Markov-switching classification technique to classify the coincident index into recession and expansion regimes. Finally, I use these trained classifiers to evaluate the evidence for new business cycle turning points in end-of-sample data that has not yet been classified by the NBER. Using a pseudo real-time dataset, I evaluate the ability of this three-step approach to identify new business cycle peaks and troughs in the United States since 1980.
<!--- Second, I train a variety of supervised machine learning classification techniques, as well as one unsupervised (Markov-switching) classification technique, --->


This paper is the first to systematically evaluate the ability of high frequency data to improve upon the timeliness with which new expansions and recessions can be identified. An existing literature has investigated the speed with which U.S. turning points can be identified in real time. [^7] While improving significantly on the speed with which the NBER identifies turning points, this literature has still documented significant delays in which new turning points are established. For example, @Hamilton_2011 surveys a range of statistical models that were in place to identify business cycle turning points in real time, and finds that these models did not send a definitive signal regarding the December 2007 NBER peak until late 2008. This literature has focused exclusively on data that is measured monthly and quarterly. At the same time, as discussed above, there are good reasons to think that higher frequency data can improve on the speed with which turning points can be identified. 

A significant literature has also used dynamic factor models combined with non-linear elements to empirically capture expansions and recessions in economic activity (see, e.g. @Diebold_Rudebusch_1996 and @Chauvet_1998). In this paper I extend this literature to higher frequency data, using nonlinear methods combined with dynamic factor models designed to work with mixed and high frequency data. Specifically, I work with the dynamic factor model of @Aruoba_Diebold_Scotti_2009 to extract a high frequency measure of the state of economic activity, and then use nonlinear classification techniques to identify turning points from this measure. 

This paper is organized as follows. In section 2 I describe the pseudo real-time dataset. In section 3 I describe the specification and estimation of the dynamic factor model at daily frequency. Section 4 presents results of the out-of-sampler exercise to identify turning point dates using the Markov regime switching classifier. Section 5 lays out a future plan for this study. 

# Pseudo Real-Time Dataset 

As used in @Aruoba_Diebold_Scotti_2009, I used the daily yield curve term premium defined as the difference between daily 10-year and daily 3-month U.S. Treasury yields (TY), the weekly initial claims for unemployment insurance (ICSA), the monthly nonfarm payroll employment (PAYEMS), and the quarterly real GDP (GDP) as my coincident variables. Then I created a pseudo real-time dataset on these variables. The following procedure describes how I compiled the pseudo real-time dataset. 

- Data series as of April 30, 2020 are downloaded from Federal Reserve Bank at St. Louis Economic Data (FRED) database. 
- On every Sunday starting from Jan 1, 1979, I updated coincident variables. I call Sundays the "analysis date".
<!-- In FRED, the PAYEMS data point listed on the first day of the month is the employment number for that month. For example, the PAYEMS data point on Oct 1, 2019 is the employment data for October 2019. Therefore I shifted the date by a month, using the last day of the month to be the date of PAYEMS data-->
- Following ADS, for each analysis date, I stripped TY, ICSA, PAYEMS data of a linear and a quadratic trend, and standardized the residuals. In future work I plan to evaluate a model estimated in growth rates of TY, ICSA, PAYEMS that won't require polynomial detrending. 
- The dataset is constructed to reflect reporting lags. For example, the GDP data point on Oct 1, 2019 is the GDP data for Q4 of 2019. There is another issue. On Jan 13, 1979, the GDP data for Q4 1978 would not have been released yet. In fact, the first release of Q4 1978 data wouldn't have been until late in January 1979. For example, the Q4 1978 data wouldn't be included until the analysis date is at or after Feb. 1, 1979. To fix these issues, I assume that GDP is released on the last day of the month following the quarter for which it is measuring. So Q1 data is released on April 30 for Q1, Q2 data is released on July 31, Q3 data is released on October 31, and Q4 data is released on January 31. So, for any analysis date that is past one of these dates, I include GDP for the previous quarter in the dataset. 
- Following ADS, I then stripped GDP data of a linear, a quadratic and a cubic trend, and standardized the residuals. Again, in future work I plan to evaluate a model estimated in growth rates of GDP that won't require polynomial detrending.
- I also included data lagged one period of ICSA, PAYEMS, and GDP into the dataset. 

Following @Aruoba_Diebold_Scotti_2009, I use simple first-order dynamics throughout the framework to reduce the number of parameters to be estimated. As shown below, the simple AR(1) dynamics produces promising results. 

# Methodology

## Dynamic Factor Model at Daily Frequency
 
I follow @Aruoba_Diebold_Scotti_2009 to propose a dynamic factor model at daily frequency to extract a coincident index of real economic activity using weekly, monthly and quarterly data. Let $x_t$ denote the underlying real economic activity at day $t$, which is assumed to evolve daily with $AR(1)$ dynamics described as follows, 

$$x_t = \rho x_{t-1}+e_t$$ where $e_t$ is a white noise innovation with variance $1-\rho^2$.

I use the yield curve term premium for the daily variable $y^1_t$, defined as the difference between 10-year and 3-month U.S. Treasury yields. Because the term premium is a stock variable, there are no aggregation issues. $y^1_t$ depends linearly on $x_t$ and contemporaneously and serially uncorrelated innovations $u_t$. Because the term premium is reported every weekday, its persistence is modeled at the daily frequency with a $u^1_t$ term that follows $AR(1)$ dynamics,

$$\begin{aligned}
y^1_t &= \begin{cases} \beta_1 x_t + u^1_t & \\
          NA & 
          \end{cases} \\
&= \begin{cases} \beta_1 x_t + \gamma_1 u^1_{t-1}+\zeta_t & y^1_t \text{is observed}\\
    NA & y^1_t \text{is not observed}
    \end{cases}
\end{aligned}$$ where $\xi_t$ is a white noise innovation with variance $\sigma^2_1$.  
  
I use initial claims for unemployment insurance for the weekly variable $y^2_t$. Because it is a flow variable reported on every Saturday covering the seven-day period from Sunday to Saturday, $y^2_t$ on Saturday is set to the sum of the previous seven daily values, constructed with a weekly cumulator variable $C^W_t$. To model persistence at the daily frequency, $y^2_t$ is set to depend on its previous observed value with one-week lag. Theoretically the persistence can be modeled with multiple lags of the $u^2_t$ term; however, the number of parameters need to be estimated will be unnecessarily large.

$$y^2_t = \begin{cases} \beta_2 C^W_t + \gamma_2 y_{2, t-7} + u^2_t & y^2_t \text{is observed} \\ NA & y^2_t \text{is not observed} \end{cases}$$

$$C_t^W = \xi_t^W C_{t-1}^W + x_t = \xi_t^W C_{t-1}^W + \rho x_{t-1}+e_t$$

$$\xi^W_t = \begin{cases} 
          0 &  \text{if t is the first day of a week} \\
          1 & \text{otherwise} \end{cases}$$ where $u^2_t$ is a white noise innovation with cumulated variance $7 \times \sigma^2_1$.  

I use nonfarm payroll employment for the monthly variable $y^3_t$. Because it is a monthly stock variable, the end-of-month value is set to the end-of-month daily value. Persistence is modeled with its observed value with one-month lag. The number of days in each month is assumed to be 30 for simplicity.  

$$y^3_t = \begin{cases} \beta_3 x_t + \gamma_3 y_{3, t-30} + u^3_t & y^3_t \text{is observed} \\
          NA & y^3_t \text{is not observed} \end{cases}$$ where $u^3_t$ is a white noise innovation with variance $\sigma^3_1$.  
          
I use real GDP for the quarterly variable $y^4_t$. Because it is a flow variable, the end-of-quarter value is set to the sum of daily values within the quarter with a quarterly cumulator variable $C^Q_t$. Persistence is modeled with its observed value with one-quarter lag. The number of days in each quarter is assumed to be 90 for simplicity.  

$$y^4_t = \begin{cases} \beta_4 C^Q_t + \gamma_4 y_{4, t-90} + u^4_t & y^4_t \text{is observed} \\
          NA & y^4_t \text{is not observed} \end{cases}$$
          
$$C_t^Q = \xi_t^Q C_{t-1}^Q + x_t = \xi_t^Q C_{t-1}^Q + \rho x_{t-1}+e_t$$

$$\xi^Q_t = \begin{cases} 
          0 &  \text{if t is the first day of a quarter} \\
          1 & \text{otherwise} \end{cases}$$ where $u^4_t$ is a white noise innovation with cumulated variance $90 \times \sigma^4_1$. 

All $y^i_t$ are demeaned and detrended. This completes the specification of the model. The dynamic factor model at daily frequency model is cast as follows. 

$$\underbrace{\begin{bmatrix} 
    y^1_t \\
    y^2_t \\
    y^3_t \\
    y^4_t
  \end{bmatrix}}_{\boldsymbol{\Upsilon_t}} = 
  \underbrace{\begin{bmatrix} 
    \gamma_1 & \beta_1 & 0 & 0 & 0 \\
    0 & 0 & \beta_2 & 0 & \gamma_2 \times y_{2, t-7} \\
    0 & \beta_3 & 0 & 0 & \gamma_3 \times y_{3,t-30} \\
    0 & 0 & 0 & \beta_4 & \gamma_4 \times y_{4, t-90}
  \end{bmatrix}}_{\boldsymbol{FF_t}} \times 
  \underbrace{\begin{bmatrix} 
    u^1_{t-1} \\
    x_t \\
    C^W_t \\
    C^Q_t \\
    1 
  \end{bmatrix}}_{\boldsymbol{\theta_t}} +
  \underbrace{\begin{bmatrix}
  \zeta_t \\
    u^2_t \\
    u^3_t \\
    u^4_t
  \end{bmatrix}}_{\boldsymbol{\nu_t}}$$

$$\underbrace{\begin{bmatrix}
    u^1_{t-1} \\
    x_t \\
    C^W_t \\
    C^Q_t \\
    1
  \end{bmatrix}}_{\boldsymbol{\theta_t}} =
  \underbrace{\begin{bmatrix}
    \gamma_1 & 0 & 0 & 0 & 0 \\
    0 & \rho & 0 & 0 & 0\\
    0 & \rho & \xi^W_t & 0 & 0 \\
    0 & \rho & 0 & \xi^Q_t & 0 \\
    0 & 0 & 0 & 0 & 1
  \end{bmatrix}}_{\boldsymbol{GG_t}} \times
  \underbrace{\begin{bmatrix}
    u^1_{t-2} \\
    x_{t-1} \\
    C^W_{t-1} \\
    C^Q_{t-1} \\
    1
  \end{bmatrix}}_{\boldsymbol{\theta_{t-1}}} +
  \underbrace{\begin{bmatrix}
    \zeta_{t-1} \\
    e_t \\
    e_t \\
    e_t \\
    0
  \end{bmatrix}}_{\boldsymbol{\omega_t}}$$
  
$$\underbrace{\begin{bmatrix}
    \zeta_t \\
    u^2_t \\
    u^3_t \\
    u^4_t\\
  \end{bmatrix}}_{\boldsymbol{\nu_t}} \sim
  N \Bigg( \begin{bmatrix} 0 \\ 0\\ 0\\ 0\\ \end{bmatrix}, 
  \underbrace{\begin{bmatrix} {\sigma_1}^2 & 0 & 0 & 0 \\ 0 & 7 \times {\sigma_2}^2 & 0 & 0 \\
0 &  0 & {\sigma_3}^2 & 0 \\ 0 & 0 & 0 & 90 \times {\sigma_4}^2 \end{bmatrix} \Bigg)}_{\boldsymbol{V_t}}$$

$$\underbrace{\begin{bmatrix}
    \zeta_{t-1} \\
    e_t \\
    e_t \\
    e_t \\
    0
  \end{bmatrix}}_{\boldsymbol{\omega_t}} \sim
  N \Bigg( \begin{bmatrix} 0 \\ 0\\ 0\\ 0\\ 0\\ \end{bmatrix}, 
  \underbrace{\begin{bmatrix} {\sigma_1}^2 & 0 & 0 & 0 & 0\\ 0 & 1-\rho^2 & 0 & 0 & 0 \\
0 &  0 & 1-\rho^2 & 0 & 0 \\ 0 & 0 & 0 & 1-\rho^2&0 \\ 0 & 0 & 0 & 0 & 0 \end{bmatrix} \Bigg)}_{\boldsymbol{W_t}}$$

The model can be represented in time-varying state-space form as

$$\begin{aligned}
\boldsymbol{\Upsilon_t} &= \boldsymbol{FF_t} \times \boldsymbol{\theta_t} + \boldsymbol{\nu_t} \\
\boldsymbol{\theta_t} &= \boldsymbol{GG_t} \times \boldsymbol{\theta_{t-1}} + \boldsymbol{\omega_t} 
\end{aligned}$$ where $\boldsymbol{\Upsilon_t}$ is a vector of variables that are subject to missing values, $\boldsymbol{\theta_t}$ is a vector of state variables, $\boldsymbol{\nu_t}$ and $\boldsymbol{\omega_t}$ are vectors of measurement and transition shocks.

Following ADS, I use the Kalman filter and smoother to obtain optimal extractions of the latent state of real economic activity. At each analysis date, parameters are re-estimated. As is standard for classical estimation, I initialize the Kalman filter using the unconditional mean and covariance matrix of the state vector. Parameters are estimated with maximum likelihood methods.

As an example, estimated factor from the January 6, 1979 and April 30, 2020 analysis dates are shown in Figure \@ref(fig:ch3-factor). Shaded areas indicate U.S. recessions. The factor drops during recessions. The sharp decline since late March 2020 presents the severe economic impact of Covid-19 pandemic. 

```{r ch3-factor, echo=FALSE, fig.cap='Latent Real Economic Activity Factor at Daily Frequency', fig.align='center', fig.width=8.5, out.width = "100%"}
knitr::include_graphics(c("/Users/lixiang/Desktop/Mixed-frequency/Prospectus/Figs/Ch3/Factor-plot.png"))
```

## Supervised Regime Classifications

$\hat{x_t}$ is the coincident index of real economic activity at day $t$ extracted from the dynamic factor model with mixed frequencies, using linear optimal procedures as described in the previous step. The second step is to nowcast the recession and expansion regimes of $\hat{x_t}$ at day $t$. I use a supervised (Markov-switching) classification technique to classify the coincident index into recession and expansion regimes. 

In this case, supervised classification techniques require NBER turning point dates to be known. The NBER turning point dates are therefore taken as given that it correctly classify the unobservable state of the economy into either regime. Markov-switching classification technique models the differences between regimes using a mixture of normal distributions. A large amount of the literature has emphasized that expansions and recessions are probabilistically different, and out-of-sample real-time dating used by this approach has turned out to be promising. @Chauvet_Piger_2008 use a monthly real-time dataset and show that the dynamic factor Markov-switching model identifies NBER dates more accurately and identifies troughs with a larger lead than a nonparametric algorithm. @Camacho_Perez_Poncela_2018 use mixed-frequency dataset at monthly and quarterly frequencies, and extend the Markov-switching dynamic factor model to monitor economic activity on a monthly basis. Compared with a balanced panel of indicators, @Camacho_Perez_Poncela_2018 obtain substantial improvements in producing real-time business cycle probabilities. 

The transition between regimes is driven by a Markov process with $p_{ji} = Pr(S_t=j|S_{t-1} = i)$, which is the transition probability of $S_t$ switching from regime $i$ to regime $j$. @Camacho_Perez_Poncela_2015 and @Owyang_Piger_Wall_2005 found that the Markov-switching AR(0) model provided accurate and robust identification of NBER business cycle dates. I fit the first difference of the coincident index, which is denoted $\Delta\hat{x_t}$, to a univariate Markov-switching AR(0) process with a switching mean. 

$$\begin{aligned}
\Delta\hat{x_t} &= \beta_{S_t} + \epsilon_t\\
\epsilon_t &\sim N(0, \sigma^2)
\end{aligned}$$

<!---
The prior takes the form: 

$$\begin{aligned}
p(\beta, P) &= p(\beta) \prod_{i=0}^{1} p(p_{00})p(p_{11})
\end{aligned}$$ where 

$$\begin{aligned}
\beta &\sim N(\mu, V) \\
p_{00} &\sim Beta(\alpha^0_1, \alpha^0_2) \\
p_{11} &\sim Beta(\alpha^1_1, \alpha^1_2) 
\end{aligned}$$
--->

The parameters of the model are $\Omega = (\beta_0, \beta_1, p_{00}, p_{11}, \sigma)'$. Let $S_t \in {0,1}$, where $S_t = 0$ indicates that day $t$ is a expansion regime, and $S_t =1$ indicates that day $t$ is a recession regime. The model estimates probabilities of a recession regime on day $t$ conditional on data available on day $T$, denoted $\hat{P}(S_t = 1 |\Psi_T)$. 

I estimate the Markov regime switching model using a non-parametric technique. It directly ties the estimates to the NBER regimes, which is what I am trying to nowcast. I estimate $\beta_0$ and $\beta_1$ as the mean of $\Delta\hat{x_t}$ in each NBER regime, and the estimated transition probabilities are the mean of the transitions using the NBER regimes. The variance of the disturbance terms are estimated from the residuals of this regression. Because NBER recession and expansion dates are known only with a substantial lag, I do not use the NBER indicator through the end of the relevant sample to estimate parameters at each analysis date. Instead I adopt a conservative approach and estimate model parameters on data ending one year prior to the analysis date. Then, given these parameters, the filter developed in @Hamilton_1989 is run through to the end of the data available at the analysis date in order to get the recession probabilities. 

<!---The recession probability nowcasted on Jan 06, 1979 and on April 30, 2020 are shown in Figure \@ref(fig:ch3-prob). 

```{r ch3-prob, echo=FALSE, fig.cap='Recession Probabilities at Daily Frequency', fig.align='center', fig.width=8.5, out.width = "100%"}
knitr::include_graphics(c("/Users/lixiang/Desktop/Mixed-frequency/Prospectus/Figs/Ch3/Recession-prob-plot.png"))
```
--->

<!--- parametric estimation of parameters here: Parameters of the univariate Markov-switching model are estimated via Bayesian samplers, following @Kim_Nelson_1999. The estimation also produces the posterior probability of a recession.---> 

## Business Cycle Phases Dating Procedures

Finally, I use the Markov-switching classifier to evaluate the evidence for new business cycle turnings points in end-of-sample data that has not yet been classified by the NBER. In order to convert recession probabilities $\hat{P}(S_t = 1 |\Psi_T)$ into a binary variable that defines whether the economy is in an expansion or a recession regime on day $t$, and whether a new peak or trough can be confirmed to occur on day $t$, I use a simple procedure. 

Specifically, suppose that the last NBER turning point date that was announced is a business cycle trough, followed by periods of known expansions. Denoting the analysis date as $T$, I then look for the first value of $T$ for which the average value of $P(S_t=1|\Psi_T)$ over the 12 weeks prior to the analysis date exceeds 0.8. I consider this a recession "call" or a peak "call", and the associated value of $T$ is the first analysis date for which this new recession or this new peak is identified. Alternatively, suppose that the last NBER turning point date that was announced is a business cycle peak, followed by periods of known recessions. Then I look for the first value of $T$ for which the average value of $P(S_t=1|\Psi_T)$ over the 12 weeks prior to the analysis date is below 0.2. I consider this a expansion "call" or a trough "call" and the associated value of $T$ is the first analysis date for which this new expansion or this new trough is identified. This procedure mirrors that in @Chauvet_Piger_2008 for monthly data. Having elements of my model specification be consistent with existing literature is useful to compare my results to this literature. I also produce results for an alternative threshold of 0.9 as a robustness check. 

<!---Based on the existing literature, a peak is identified the first time I see the average of the last 12 weeks of recession probabilities exceed 0.8 and a trough is identified the first time I see the average of the last 12 weeks of recession probabilities below 0.2. I also use the threshold of 0.9 as a robustness check, and identify a peak the first time the average of the last 12 weeks of probabilities exceed 0.9 and a trough the first time the average of the last 12 weeks of probabilities below 0.1. --->

<!---
In the first step, for each analysis date $t$, if the average probability of recession over the last 30 days is larger than 80%, a "recession" regime is designated for day $t$; if the average probability of recession over the last 30 days is smaller than 20%, an "expansion" regime is designated for day $t$. That is, $\bar{\hat{P}}(S_{t-k} = 1 |\Psi_T) \geq 0.8, \forall k = 0, ... 29$ defines a recession regime of the economy on day $t$, and $\bar{\hat{P}}(S_{t-k} = 1 |\Psi_T) \leq 0.2, \forall k = 0, ... 29$ defines an expansion regime of the economy on day $t$.

In the second step, for each analysis date $t$, if the current regime is an expansion, a new peak is called after observing 8 consecutive weeks of probabilities exceeding 80% for the previous month; if the current regime is a recession, a new trough is called after observing 8 consecutive weeks of probabilities below 20% for the previous month. In other words, 8 weeks of confirmation are required before definitely saying there is a new peak or trough. 
--->

# Preliminary Baseline Results

Using a pseudo real-time dataset, I evaluate the ability of my approach to identify new business cycle peaks and troughs in the United States since 1980. Table \@ref(tab:ch3-table-peak1) compares the real-time performance of the Dynamic Factor Markov Switching Model at Daily Frequency (DFMSDF) for detecting business cycle peaks in real time. The table also compares these results to those from @Giusto_Piger_2017, which uses only monthly data. Table \@ref(tab:ch3-table-trough1) presents the analogous results for troughs. The first column of the table shows the monthly dates of NBER turning points. The second column shows the starting date of NBER business cycles phases, which is defined as the first date after NBER turning points occur. The third column shows dates when turning points are called using DFMSDF, following the proposed simple procedure. The fourth column shows the number of lag days in NBER announcements made by the NBER's Business Cycle Dating Committee. The fifth column shows the number of lag days using a learning vector quantization method as in @Giusto_Piger_2017. The sixth column shows the number of lag days using the DFMSDF method proposed in this chapter. 

The tables shows that the DFMSDF method identifies all NBER peaks and troughs over the out of sample period. All of NBER turning points dates in the first column are identified by the DFMSDF method. The tables also suggest that the DFMSDF method is very fast in identifying NBER turning points dates in the first column. The fourth and fifth column show that NBER's Business Cycle Dating Committee and @Giusto_Piger_2017 produce NBER turning points dates with a significant delay. Except for the January 1980 peak, the sixth columns shows that the peak and trough calls made by the DFMSDF approach are quicker than those by the NBER's Business Cycle Dating Committee and by @Giusto_Piger_2017. The DFMSDF method calls the January 1980 peak faster than the NBER's Business Cycle Dating Committee and only a few days slower than @Giusto_Piger_2017. 

In cases of the July 1990 peak, the March 2001 peak, the December 2007 peak, the November 2001 trough and the June 2009 trough, the DFMSDF method predicts business cycle phases prior to their starting dates. This striking result shows that the DFMSDF algorithm not only nowcasts recessions and expansions timely, but might also be powerful in forecasting recessions and expansions. 

On average, NBER's Business Cycle Dating Committee and @Giusto_Piger_2017 establish business cycle peaks with delay of 224 and 134 days respectively; however, the DFMSDF method established the business cycle peak 21 days prior to it starts. Both NBER's Business Cycle Dating Committee and @Giusto_Piger_2017 are slower in identifying business cycle troughs, with an average delay of 446 and 234 days respectively, while the DFMSDF method identifies the business cycle trough with an average lead of 11 days.

The NBER has not announced any new recessions since the 2007-2009 Great Recession. However, the DFMSDF method identified a new peak on March 15, 2020. The new recession is identified as the Covid-19 public health crisis started, which has not yet been classified by the NBER. 

```{r ch3-table-peak1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
library(dplyr)
library(kableExtra)

options(knitr.kable.NA = '')

peak_table <- data.frame(matrix(ncol = 6, nrow = 7))
colnames(peak_table) <- c("NBER Peak Date", "First Day of Recession", "Date Peak Call Available - DFMSDF", "NBER's Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag", "DFMSDF Lag" )

peak_table[1,1] <- c("Jan-1980")
peak_table[2,1] <- c("Jul-1981")
peak_table[3,1] <- c("Jul-1990")
peak_table[4,1] <- c("Mar-2001")
peak_table[5,1] <- c("Dec-2007")
peak_table[6,1] <- c("Average")

peak_table[1,2] <- c("2/1/1980")
peak_table[2,2] <- c("8/1/1981")
peak_table[3,2] <- c("8/1/1990")
peak_table[4,2] <- c("4/1/2001")
peak_table[5,2] <- c("1/1/2008")

peak_table[1,3] <- c("5/11/1980")
peak_table[2,3] <- c("11/1/1981")
peak_table[3,3] <- c("7/29/1990")
peak_table[4,3] <- c("7/2/2000")
peak_table[5,3] <- c("12/9/2007")
peak_table[7,3] <- c("3/15/2020")

peak_table[1,4] <- c("123")
peak_table[2,4] <- c("158")
peak_table[3,4] <- c("267")
peak_table[4,4] <- c("239")
peak_table[5,4] <- c("335")
peak_table[6,4] <- c("224")

peak_table[1,5] <- c("92")
peak_table[2,5] <- c("126")
peak_table[3,5] <- c("78")
peak_table[4,5] <- c("216")
peak_table[5,5] <- c("158")
peak_table[6,5] <- c("134")

peak_table[1,6] <- c("100")
peak_table[2,6] <- c("92")
peak_table[3,6] <- c("-3")
peak_table[4,6] <- c("-273")
peak_table[5,6] <- c("-23")
peak_table[6,6] <- c("-21")

kable(peak_table, "latex", booktabs = T, caption = "U.S. Business Cycle Peaks Identified in Real-time (Threshold=0.8)") %>%
  kable_styling(full_width = F, font_size = 9, position = "center", latex_options = "hold_position") %>%
  column_spec(3:4, width = "7em") %>%
  column_spec(5:6, width = "6em") %>%
  column_spec(1:2, width = "6em") %>%
  row_spec(6, bold = T) 
# %>%
#   footnote(general = "Rule: 0.8 for three months")
```

```{r ch3-table-trough1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
options(knitr.kable.NA = '')

trough_table <- data.frame(matrix(ncol = 6, nrow = 6))
colnames(trough_table) <- c("NBER Trough Date", "First Day of Expansion", "Date Trough Call Available - DFMSDF", "NBER's Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag", "DFMSDF Lag")

trough_table[1,1] <- c("Jul-1980")
trough_table[2,1] <- c("Nov-1982")
trough_table[3,1] <- c("Mar-1991")
trough_table[4,1] <- c("Nov-2001")
trough_table[5,1] <- c("Jun-2009")
trough_table[6,1] <- c("Average")

trough_table[1,2] <- c("8/1/1980")
trough_table[2,2] <- c("12/1/1982")
trough_table[3,2] <- c("4/1/1991")
trough_table[4,2] <- c("12/1/2001")
trough_table[5,2] <- c("7/1/2009")

trough_table[1,3] <- c("8/10/1980")
trough_table[2,3] <- c("12/12/1982")
trough_table[3,3] <- c("6/2/1991")
trough_table[4,3] <- c("8/19/2001")
trough_table[5,3] <- c("5/31/2009")

trough_table[1,4] <- c("341")
trough_table[2,4] <- c("219")
trough_table[3,4] <- c("631")
trough_table[4,4] <- c("593")
trough_table[5,4] <- c("446")
trough_table[6,4] <- c("446")

trough_table[1,5] <- c("127")
trough_table[2,5] <- c("136")
trough_table[3,5] <- c("443")
trough_table[4,5] <- c("308")
trough_table[5,5] <- c("157")
trough_table[6,5] <- c("234")

trough_table[1,6] <- c("9")
trough_table[2,6] <- c("11")
trough_table[3,6] <- c("62")
trough_table[4,6] <- c("-104")
trough_table[5,6] <- c("-31")
trough_table[6,6] <- c("-11")

kable(trough_table, "latex", booktabs = T, caption = "U.S. Business Cycle Troughs Identified in Real-time (Threshold=0.8)") %>%
  kable_styling(full_width = F, font_size = 9, position = "center", latex_options = "hold_position") %>%
  column_spec(3:4, width = "7em") %>%
  column_spec(5:6, width = "6em") %>%
  column_spec(1:2, width = "6em") %>%
  row_spec(6, bold = T) 
# %>%
#   footnote(general = "Rule: 0.8 for three months")
```

However, as Table \@ref(tab:ch3-table-false1) shows, the DFMSDF method generates a number of false positives and false negatives. The baseline dating procedure produces six false recessions and one false expansions. However, these are all short in that they only send a false signal for a few weeks. In future work I plan to incorporate several approaches to reduce the number of false troughs and peaks, including incorporating additional series into the analysis. 

```{r ch3-table-false1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
options(knitr.kable.NA = '')

false_table <- data.frame(matrix(ncol = 2, nrow = 6))
colnames(false_table) <- c("False Recessions", "False Expansions")

false_table[1,1] <- c("2/16/1979-12/30/1979")
false_table[2,1] <- c("10/7/1984-11/11/1984")
false_table[3,1] <- c("1/24/1988-2/7/1988")
false_table[4,1] <- c("7/9/1989-8/6/1989")
false_table[5,1] <- c("4/23/1995-6/18/1995")
false_table[6,1] <- c("4/13/2003-5/4/2003")

false_table[1,2] <- c("2/28/1982-3/14/1982")

kable(false_table, "latex", booktabs = T, caption = "False Recessions and False Expansions Identified in Real-time (Threshold=0.8)") %>%
  kable_styling(full_width = F, font_size = 9, position = "center", latex_options = "hold_position") 
# %>%
#   footnote(general = "Rule: 0.8 for three months")
```

Table \@ref(tab:ch3-table-peak2), Table \@ref(tab:ch3-table-trough2) and Table \@ref(tab:ch3-table-false2) show the result using the threshold of 0.9 as a robustness check. The results are qualitatively similar to those for the 0.8 threshold, although no false expansions are identified in this case. 

```{r ch3-table-peak2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
options(knitr.kable.NA = '')

peak_table <- data.frame(matrix(ncol = 6, nrow = 7))
colnames(peak_table) <- c("NBER Peak Date", "First Day of Recession", "Date Peak Call Available - DFMSDF", "NBER's Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag", "DFMSDF Lag" )

peak_table[1,1] <- c("Jan-1980")
peak_table[2,1] <- c("Jul-1981")
peak_table[3,1] <- c("Jul-1990")
peak_table[4,1] <- c("Mar-2001")
peak_table[5,1] <- c("Dec-2007")
peak_table[6,1] <- c("Average")

peak_table[1,2] <- c("2/1/1980")
peak_table[2,2] <- c("8/1/1981")
peak_table[3,2] <- c("8/1/1990")
peak_table[4,2] <- c("4/1/2001")
peak_table[5,2] <- c("1/1/2008")

peak_table[1,3] <- c("5/28/1980")
peak_table[2,3] <- c("11/22/1981")
peak_table[3,3] <- c("8/12/1990")
peak_table[4,3] <- c("7/9/2000")
peak_table[5,3] <- c("12/23/2007")
peak_table[7,3] <- c("3/15/2020")

peak_table[1,4] <- c("123")
peak_table[2,4] <- c("158")
peak_table[3,4] <- c("267")
peak_table[4,4] <- c("239")
peak_table[5,4] <- c("335")
peak_table[6,4] <- c("224")

peak_table[1,5] <- c("92")
peak_table[2,5] <- c("126")
peak_table[3,5] <- c("78")
peak_table[4,5] <- c("216")
peak_table[5,5] <- c("158")
peak_table[6,5] <- c("134")

peak_table[1,6] <- c("114")
peak_table[2,6] <- c("113")
peak_table[3,6] <- c("-11")
peak_table[4,6] <- c("-266")
peak_table[5,6] <- c("-9")
peak_table[6,6] <- c("-7")

kable(peak_table, "latex", booktabs = T, caption = "U.S. Business Cycle Peaks Identified in Real-time (Threshold=0.9)") %>%
  kable_styling(full_width = F, font_size = 9, position = "center", latex_options = "hold_position") %>%
  column_spec(3:4, width = "7em") %>%
  column_spec(5:6, width = "6em") %>%
  column_spec(1:2, width = "6em") %>%
  row_spec(6, bold = T) 
# %>%
#   footnote(general = "Rule: 0.9 for three months")
```

```{r ch3-table-trough2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
options(knitr.kable.NA = '')

trough_table <- data.frame(matrix(ncol = 6, nrow = 6))
colnames(trough_table) <- c("NBER Trough Date", "First Day of Expansion", "Date Trough Call Available - DFMSDF", "NBER's Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag", "DFMSDF Lag")

trough_table[1,1] <- c("Jul-1980")
trough_table[2,1] <- c("Nov-1982")
trough_table[3,1] <- c("Mar-1991")
trough_table[4,1] <- c("Nov-2001")
trough_table[5,1] <- c("Jun-2009")
trough_table[6,1] <- c("Average")

trough_table[1,2] <- c("8/1/1980")
trough_table[2,2] <- c("12/1/1982")
trough_table[3,2] <- c("4/1/1991")
trough_table[4,2] <- c("12/1/2001")
trough_table[5,2] <- c("7/1/2009")

trough_table[1,3] <- c("8/17/1980")
trough_table[2,3] <- c("12/19/1982")
trough_table[3,3] <- c("6/9/1991")
trough_table[4,3] <- c("8/26/2001")
trough_table[5,3] <- c("5/31/2009")

trough_table[1,4] <- c("341")
trough_table[2,4] <- c("219")
trough_table[3,4] <- c("631")
trough_table[4,4] <- c("593")
trough_table[5,4] <- c("446")
trough_table[6,4] <- c("446")

trough_table[1,5] <- c("127")
trough_table[2,5] <- c("136")
trough_table[3,5] <- c("443")
trough_table[4,5] <- c("308")
trough_table[5,5] <- c("157")
trough_table[6,5] <- c("234")

trough_table[1,6] <- c("16")
trough_table[2,6] <- c("18")
trough_table[3,6] <- c("69")
trough_table[4,6] <- c("-97")
trough_table[5,6] <- c("-31")
trough_table[6,6] <- c("-5")

kable(trough_table, "latex", booktabs = T, caption = "U.S. Business Cycle Troughs Identified in Real-time (Threshold=0.9)") %>%
  kable_styling(full_width = F, font_size = 9, position = "center", latex_options = "hold_position") %>%
  column_spec(3:4, width = "7em") %>%
  column_spec(5:6, width = "6em") %>%
  column_spec(1:2, width = "6em") %>%
  row_spec(6, bold = T) 
# %>%
#   footnote(general = "Rule: 0.9 for three months")
```

```{r ch3-table-false2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
options(knitr.kable.NA = '')

false_table <- data.frame(matrix(ncol = 2, nrow = 6))
colnames(false_table) <- c("False Recessions", "False Expansions")

false_table[1,1] <- c("2/16/1979-12/23/1979")
false_table[2,1] <- c("10/14/1984-11/4/1984")
false_table[3,1] <- c("1/24/1988-2/7/1988")
false_table[4,1] <- c("7/16/1989-7/23/1989")
false_table[5,1] <- c("4/30/1995-6/11/1995")
false_table[6,1] <- c("4/20/2003-4/27/2003")

kable(false_table, "latex", booktabs = T, caption = "False Recessions and False Expansions Identified in Real-time (Threshold=0.9)") %>%
  kable_styling(full_width = F, font_size = 9, position = "center", latex_options = "hold_position") 
#%>% footnote(general = "Rule: 0.9 for three months")
```

# Future Directions

## A Broader Vintage Real-Time Dataset 

I plan to incorporate more monthly variables into the dataset, such as the unemployment rate, industrial production, and real manufacturing and trade sales. This will require modifying the DFMSDF method to incorporate additional series. I also plan to explore the possibility of growth rate specification of the DFMSDF method. 

The current data set incorporates data revisions, since it was obtained in April 2020. Therefore I plan to compile the vintage data into a real-time dataset that would have been available on each day. By using the vintage real-time dataset, I search for new business phases turning points as if I were an analyst each day beginning on Jan 1, 1979.

## Supervised Regime Classifications

I plan to train a variety of supervised machine learning classification techniques. @Giusto_Piger_2017 propose a learning vector quantization approach to nowcast the newest five U.S. recessions in real time. Their approach identifies the dates accurately, with no false positives. Compared with the dynamic factor Markov-switching models, their technique identifies peaks and troughs at an impressive speed. In a recent paper, @Piger_forthcoming compares the timely performance of a wide range of classifiers to nowcasting the U.S. expansion and recession phases at monthly frequency, and finds that the k-nearest neighbor classifier and the random forest classifier are quick to identify turning points while producing no false positives for a narrow data set. 

An as example, the Naive Bayes classifier estimates the conditional recession probability at day $t$ as: 

$$Pr(S_t = 0| \hat{\theta_t}) = \frac{f(\hat{\theta_t}|S_t = 0) Pr(S_t =0)}{f(\hat{\theta_t}|S_t = 0)Pr(S_t =0) + f(\hat{\theta_t}|S_t = 1)Pr(S_t =1)},$$ where $f(\hat{\theta_t}|S_t = c)$ can be evaluated from the training sample using a normal distribution or a kernel density estimator. 

Another example is the k-nearest neighbor classifier, which assumes that similar things exist in close Euclidean distance. Let $A$ be the set consisting of the $k$ points in the training data that are closest to $\hat{\theta_t}$. The conditional recession probability at day $t$ is: 

$$Pr(S_t = 0| \hat{\theta_t}) = \frac{1}{k} \sum_{i \in A} I(S_i = 0).$$ 

In addition to the Naive Bayes classifier and the k-nearest neighbor classifier, I plan to investigate a variety of other supervised machine learning classifiers, including learning vector quantization, random forest, and boosting. Following @Piger_forthcoming, I set a rule to transform probabilities into turning points dates. 

[^7]: See for example, @Chauvet_Piger_2008, @Chauvet_Hamilton_2006, @Fossatti_2016, and  @Giusto_Piger_2017.



\newpage
# Reference
