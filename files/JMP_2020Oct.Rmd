---
title: "Nowcasting Business Cycle Phases with High and Mixed Frequency Data"
subtitle: ""
author: "Xiang LI"
date: "Oct 15, 2020"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      countIncrementalSlides: false
    css: ['default', 'metropolis', 'metropolis-fonts']
---

<style>
  .large { font-size: 200% }
  .small { font-size: 50% }
</style>

<style type="text/css">
.remark-slide-content {
    font-size: 21px;
    padding: 1em 1em 1em 1em;
}

.remark-slide-content h1 {
  font-size: 32px;
}

.remark-slide-content h2 {
  font-size: 25px;
}

.remark-slide-content h3 {
  font-size: 24px;
}

h1 {
  font-weight: normal;
  margin-top: -60px;
  margin-left: -00px;
  color: #FAFAFA;
}

</style>


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
library(tidyverse)
library(plotly)
library(kableExtra)
library(ggplot2)
library(lubridate)
library(dplyr)
library(gganimate)
library(readxl)
library(viridis)
library(viridisLite)
library(transformr)
library(magick)
library(htmlTable)
library(scales)
```


# Introduction 

- The business cycle phase consists of alternating periods of expansions and recessions.

--

- However, business cycle phases are not explicitly observed, and identification of business cycle turning points generally occurs long after the fact.

--

  - The December 2007 peak of the Great Recession was not announced by the National Bureau of Economic Research (NBER) until December 1, 2008.

  - During the Covid-19 pandemic, the NBER announced on June 8, 2020 that a new recession started in the U.S. in March 2020, with a lag of more than three months.

--

- Given the substantial costs on individuals and firms, it would be useful to have more timely identification of turning points for policymakers, firms, consumers, and financial markets participants.


---
# Introduction 

- An active literature has worked to develop statistical techniques to “nowcast” business cycle turning points toward the end of the observable sample period.

--

  - Chauvet & Piger (2008), Chauvet & Hamilton (2006), Fossati (2016), Giusto & Piger (2017)

--

- These papers use relatively low frequency data (monthly) to nowcast turning points.

--

- My paper is the first to systematically evaluate the ability of high frequency data (daily, weekly) to improve upon the timeliness with which new expansions and recessions can be identified


---
# Methodology

**Step #1: Constructing a Real-Time Dataset**

- Every Sunday starting from Jan 1, 1979 to August 31, 2020 is my "analysis date" to conduct the nowcasting exercise.

  - The plot shows the first and the last analysis date, as well as some analysis dates in between. 
  
```{r , echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
knitr::include_graphics(c("/Users/lixiang/Desktop/Mixed-frequency/JMP/Figs/Ch3/Timeline2.png"))
```


---
# Methodology

**Step #1: Constructing a Real-Time Dataset**

- Economic data for past observation periods are revised as more accurate estimates become available.

- For example, the value of the initial claims for unemployment insurance (ICSA) on May 23, 2009 downloaded from the Federal Reserve Economic Data (FRED) database on August 31, 2020 is 606,000, which is not available for a researcher on May 28, 2009.

```{r ch3-table-timeline, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(dplyr)
library(kableExtra)

options(knitr.kable.NA = '')

timeline_table1 <- data.frame(matrix(ncol = 7, nrow =2 ))
colnames(timeline_table1) <- c("Announcement", rep("Revision", times=6))
row.names(timeline_table1) <- c("Date", "Value")

timeline_table1 [1,1] <- c("2009-05-28")
timeline_table1 [1,2] <- c("2009-06-04")
timeline_table1 [1,3] <- c("2010-03-25")
timeline_table1 [1,4] <- c("2011-03-31")
timeline_table1 [1,5] <- c("2012-03-29")
timeline_table1 [1,6] <- c("2013-03-28")
timeline_table1 [1,7] <- c("2014-04-03")

timeline_table1 [2,1] <- c(623000)
timeline_table1 [2,2] <- c(625000)
timeline_table1 [2,3] <- c(611000)
timeline_table1 [2,4] <- c(612000)
timeline_table1 [2,5] <- c(608000)
timeline_table1 [2,6] <- c(607000)
timeline_table1 [2,7] <- c(606000)

kable(timeline_table1, booktabs = T) %>%
  kable_styling(full_width = T, font_size = 12, position = "center")
```


---
# Methodology

**Step #1: Constructing a Real-Time Dataset**

- **Vintage data** enables researchers to reproduce research and build more accurate forecasting models using the data available at the time.

```{r , echo=FALSE, message=FALSE, warning=FALSE, results='asis', out.width = "90%"}
knitr::include_graphics(c("/Users/lixiang/Desktop/Mixed-frequency/JMP/Figs/Ch3/Timeline.png"))
```


---
# Methodology

**Step #1: Constructing a Real-Time Dataset**

- To compile the real-time dataset, I use the **vintage data** provided by the Archive FRED (ALFRED) of the following series: 

  - Daily yield curve term premium, defined as the difference between daily 10-year and daily 3-month U.S. Treasury yields 
  - Weekly initial claims for unemployment insurance
  - Monthly nonfarm payroll employment
  - Quarterly real GDP
  
--

- Then I stripped each series of a linear and a quadratic trend, and standardized the residuals



---
# Methodology

**Step #2: Dynamic Factor Model at Daily Frequency** 

- Following Aruoba, Diebold, & Scotti (2009), I propose a dynamic factor model at daily frequency to extract $x_t$, which is the underlying real economic activity factor on day $t$

--

- $x_t$: assumed to evolve daily with AR(1) dynamics: 

$$x_t = \rho x_{t-1}+e_t$$

- $y^1_t$: term premium (daily), a stock variable

$$\begin{aligned}
y^1_t &= \begin{cases} \beta_1 x_t + u^1_t & \\
          NA & 
          \end{cases} \\
&= \begin{cases} \beta_1 x_t + \gamma_1 u^1_{t-1}+\zeta_t & y^1_t \text{is observed}\\
    NA & y^1_t \text{is not observed}
    \end{cases}
\end{aligned}$$

???

- **Because the term premium is a stock variable, there are no aggregation issues**

- **$y^1_t$ depends linearly on $x_t$**

- **persistence of $y^1_t$ is modeled at the daily frequency with a $u_t$ term that follows AR(1) dynamics**

---
# Methodology

**Step #2: Dynamic Factor Model at Daily Frequency** 

- $y^2_t$: initial claims for unemployment insurance (weekly), a flow variable

$$y^2_t = \begin{cases} \beta_2 C^W_t + \gamma_2 y_{2, t-7} + u^2_t & y^2_t \text{is observed} \\ NA & y^2_t \text{is not observed} \end{cases}$$

$$C_t^W = \xi_t^W C_{t-1}^W + x_t = \xi_t^W C_{t-1}^W + \rho x_{t-1}+e_t$$

$$\xi^W_t = \begin{cases} 
          0 &  \text{if t is the first day of a week} \\
          1 & \text{otherwise} \end{cases}$$

???

- **Because $y^2_t$ is a flow variable reported on every Saturday, $y^2_t$ on Saturday is set to the sum of the previous seven daily values, constructed with a weekly cumulator variable $C^W_t$**

- **To model persistence at the daily frequency, $y^2_t^ is set to depend on its previous observed value $y_{2, t-7}$**

- Theoretically the persistence can be modeled with multiple lags of the $u_t$ term; however, the number of parameters need to be estimated will be unnecessarily large


---
# Methodology

**Step #2: Dynamic Factor Model at Daily Frequency** 

- $y^3_t$: nonfarm payroll employment (monthly), a stock variable

$$y^3_t = \begin{cases} \beta_3 x_t + \gamma_3 y_{3, t-30} + u^3_t & y^3_t \text{is observed} \\
          NA & y^3_t \text{is not observed} \end{cases}$$
   
   
???

- **Because it is a monthly stock variable, the end-of-month value is set to the end-of-month daily value**

- **Persistence is modeled with its previous observed value**


---
# Methodology

**Step #2: Dynamic Factor Model at Daily Frequency** 

- $y^4_t$: real GDP (quarterly), a flow variable

$$y^4_t = \begin{cases} \beta_4 C^Q_t + \gamma_4 y_{4, t-90} + u^4_t & y^4_t \text{is observed} \\
          NA & y^4_t \text{is not observed} \end{cases}$$
          
$$C_t^Q = \xi_t^Q C_{t-1}^Q + x_t = \xi_t^Q C_{t-1}^Q + \rho x_{t-1}+e_t$$

$$\xi^Q_t = \begin{cases} 
          0 &  \text{if t is the first day of a quarter} \\
          1 & \text{otherwise} \end{cases}$$

???

- **Because $y^4_t$ is a flow variable, the end-of-quarter value is set to the sum of daily values within the quarter with a quarterly cumulator variable $C^Q_t$**

- **Persistence is modeled with its previous observed value**



---
# Methodology

**Step #2: Dynamic Factor Model at Daily Frequency** 

$$\underbrace{\begin{bmatrix} 
    y^1_t \\
    y^2_t \\
    y^3_t \\
    y^4_t
  \end{bmatrix}}_{\boldsymbol{\Upsilon_t}} = 
  \underbrace{\begin{bmatrix} 
    \gamma_1 & \beta_1 & 0 & 0 & 0 \\
    0 & 0 & \beta_2 & 0 & \gamma_2 \times y_{2, t-7} \\
    0 & \beta_3 & 0 & 0 & \gamma_3 \times y_{3,t-30} \\
    0 & 0 & 0 & \beta_4 & \gamma_4 \times y_{4, t-90}
  \end{bmatrix}}_{\boldsymbol{FF_t}} \times 
  \underbrace{\begin{bmatrix} 
    u^1_{t-1} \\
    x_t \\
    C^W_t \\
    C^Q_t \\
    1 
  \end{bmatrix}}_{\boldsymbol{\theta_t}} +
  \underbrace{\begin{bmatrix}
  \zeta_t \\
    u^2_t \\
    u^3_t \\
    u^4_t
  \end{bmatrix}}_{\boldsymbol{\nu_t}}$$
  
$$\underbrace{\begin{bmatrix}
    u^1_{t-1} \\
    x_t \\
    C^W_t \\
    C^Q_t \\
    1
  \end{bmatrix}}_{\boldsymbol{\theta_t}} =
  \underbrace{\begin{bmatrix}
    \gamma_1 & 0 & 0 & 0 & 0 \\
    0 & \rho & 0 & 0 & 0\\
    0 & \rho & \xi^W_t & 0 & 0 \\
    0 & \rho & 0 & \xi^Q_t & 0 \\
    0 & 0 & 0 & 0 & 1
  \end{bmatrix}}_{\boldsymbol{GG_t}} \times
  \underbrace{\begin{bmatrix}
    u^1_{t-2} \\
    x_{t-1} \\
    C^W_{t-1} \\
    C^Q_{t-1} \\
    1
  \end{bmatrix}}_{\boldsymbol{\theta_{t-1}}} +
  \underbrace{\begin{bmatrix}
    \zeta_{t-1} \\
    e_t \\
    e_t \\
    e_t \\
    0
  \end{bmatrix}}_{\boldsymbol{\omega_t}}$$


---
# Methodology

**Step #2: Dynamic Factor Model at Daily Frequency** 

$$\underbrace{\begin{bmatrix}
    \zeta_t \\
    u^2_t \\
    u^3_t \\
    u^4_t\\
  \end{bmatrix}}_{\boldsymbol{\nu_t}} \sim
  N \Bigg( \begin{bmatrix} 0 \\ 0\\ 0\\ 0\\ \end{bmatrix}, 
  \underbrace{\begin{bmatrix} {\sigma_1}^2 & 0 & 0 & 0 \\ 0 & 7 \times {\sigma_2}^2 & 0 & 0 \\
0 &  0 & {\sigma_3}^2 & 0 \\ 0 & 0 & 0 & 90 \times {\sigma_4}^2 \end{bmatrix} \Bigg)}_{\boldsymbol{V_t}}$$

$$\underbrace{\begin{bmatrix}
    \zeta_{t-1} \\
    e_t \\
    e_t \\
    e_t \\
    0
  \end{bmatrix}}_{\boldsymbol{\omega_t}} \sim
  N \Bigg( \begin{bmatrix} 0 \\ 0\\ 0\\ 0\\ 0\\ \end{bmatrix}, 
  \underbrace{\begin{bmatrix} {\sigma_1}^2 & 0 & 0 & 0 & 0\\ 0 & 1-\rho^2 & 0 & 0 & 0 \\
0 &  0 & 1-\rho^2 & 0 & 0 \\ 0 & 0 & 0 & 1-\rho^2&0 \\ 0 & 0 & 0 & 0 & 0 \end{bmatrix} \Bigg)}_{\boldsymbol{W_t}}$$


---
# Methodology

**Step #2: Dynamic Factor Model at Daily Frequency**

- The model can be represented in time-varying state-space form as:

$$\begin{aligned}
\boldsymbol{\Upsilon_t} &= \boldsymbol{FF_t} \times \boldsymbol{\theta_t} + \boldsymbol{\nu_t} \\
\boldsymbol{\theta_t} &= \boldsymbol{GG_t} \times \boldsymbol{\theta_{t-1}} + \boldsymbol{\omega_t} 
\end{aligned}$$

--
     
- Following ADS, I use the Kalman filter and smoother to obtain optimal extractions of the latent state of real economic activity $\hat{x_t}$. 
  - At each analysis date, parameters are re-estimated. 
  - As is standard for classical estimation, I initialize the Kalman filter using the unconditional mean and covariance matrix of the state vector. 
  - Parameters are estimated with maximum likelihood methods

---
# Methodology

- $\hat{x_t}$: 

![ ](animation1.gif)

???

- **Shaded areas indicate U.S. recessions**

- **The factor drops during recessions**

---
# Methodology

**Step #3: Supervised Markov Regime-Switching Classification**

- Fitting the first difference of $\hat{x_t}$ to a univariate Markov-switching $AR(0)$ process with a switching mean:

$$\begin{align}
\Delta\hat{x_t} &= \beta_{S_t} + \epsilon_t\\
\epsilon_t &\sim N(0, \sigma^2) \\
\beta_{S_t} &= \beta_0 + \beta_1 \times S_t \\
\beta_1 &<0
\end{align}$$

--

- $S_t = 1$: day $t$ is a expansion regime; $S_t =2$: day $t$ is a recession regime

--

- When $S_t$ switches from regime 1 to 2, the mean growth rate of economic activity switches from $\beta_0$ to $\beta_0 + \beta_1$

--

- $p_{ji} = Pr(S_t=j|S_{t-1} = i)$: the transition probability of $S_t$ switching from regime $i$ to regime $j$.

--

- The parameters of the model: $\Omega = (\beta_0, \beta_1, p_{11}, p_{22}, \sigma)'$

???

$\beta_1 <0$ ensures that the mean growth rate of economic activity declines.

---
# Methodology

**Step #3: Supervised (Markov-switching) Regime Classifications**

- On each analysis date $T$, using non-parametric techniques, I estimate parameters of the model using data up to one year from $T$:
  - Estimating $\beta_0$ and $\beta_1$ as the mean of $\Delta\hat{x_t}$ in each NBER regime
  - Estimating transition probabilities as the mean of transitions using the NBER regimes
  - Estimating variance of the disturbance terms as the residuals of the regression

--

- Given these estimates, I run the Hamilton smoother through data to the end of $T$ in order to obtain the recession probabilities, denoted $\hat{P}(S_t = 1 |\Psi_T)$

???

- Because NBER recession and expansion dates are known only with a substantial lag, I adopt a conservative approach and estimate model parameters on data ending one year prior to the analysis date


<!-- --- -->
<!-- # Methodology -->

<!-- - $\hat{P}(S_t = 2 |\Psi_T)$:  -->

  
---
# Methodology

**Step #4: Business Cycle Phases Dating Procedures**

- To Identify a new recession
  - Suppose that the last NBER turning point date that was announced is a business cycle trough
  - I search for analysis dates for which the average value of $\hat{P}(S_t = 1 |\Psi_T)$ over the 12 weeks prior to $T$ exceeds 0.8
  - The first analysis date is consider a recession "call"

- To identify a new expansion
  - Suppose that the last NBER turning point date that was announced is a business cycle peak
  - I search for analysis dates for which the average value of $\hat{P}(S_t = 1 |\Psi_T)$ over the 12 weeks prior to $T$ is below 0.2
  - The first analysis date is consider an expansion "call"

???

- In order to convert recession probabilities $\hat{P}(S_t = 1 |\Psi_T)$ into a recession or expansion call, I propose a simple procedure

---
# Baseline Results 

```{r ch3-table-peak1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
library(dplyr)
library(kableExtra)

options(knitr.kable.NA = '')

peak_table <- data.frame(matrix(ncol = 6, nrow = 7))
colnames(peak_table) <- c("NBER Peak Date", "First Day of Recession", "Date Recession Call Available - DFMSDF", "DFMSDF Lag", "Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag")

peak_table[1,1] <- c("Jan-1980")
peak_table[2,1] <- c("Jul-1981")
peak_table[3,1] <- c("Jul-1990")
peak_table[4,1] <- c("Mar-2001")
peak_table[5,1] <- c("Dec-2007")
peak_table[6,1] <- c("Mar-2020")
peak_table[7,1] <- c("Average")

peak_table[1,2] <- c("2/1/1980")
peak_table[2,2] <- c("8/1/1981")
peak_table[3,2] <- c("8/1/1990")
peak_table[4,2] <- c("4/1/2001")
peak_table[5,2] <- c("1/1/2008")
peak_table[6,2] <- c("3/1/2020")

peak_table[1,3] <- c("4/27/1980")
peak_table[2,3] <- c("11/1/1981")
peak_table[3,3] <- c("8/12/1990")
peak_table[4,3] <- c("7/2/2000")
peak_table[5,3] <- c("3/30/2008")
peak_table[6,3] <- c("3/22/2020")

peak_table[1,4] <- c(86)
peak_table[2,4] <- c(92)
peak_table[3,4] <- c(11)
peak_table[4,4] <- c(-273)
peak_table[5,4] <- c(89)
peak_table[6,4] <- c(21)
peak_table[7,4] <- c(5)

peak_table[1,5] <- c(123)
peak_table[2,5] <- c(158)
peak_table[3,5] <- c(267)
peak_table[4,5] <- c(239)
peak_table[5,5] <- c(335)
peak_table[6,5] <- c(99)
peak_table[7,5] <- c(204)

peak_table[1,6] <- c(92)
peak_table[2,6] <- c(126)
peak_table[3,6] <- c(78)
peak_table[4,6] <- c(216)
peak_table[5,6] <- c(158)
peak_table[6,6] <- c("NA")
peak_table[7,6] <- c(134)

peak_table %>%
  mutate_at(c("DFMSDF Lag"), function(x) {
    cell_spec(x, bold = T,
              color = spec_color(x, end = 0.9)
              )
  }) %>%
  kable(escape = F, align = "c") %>%
  kable_styling(c("striped", "condensed"), font_size = 12, full_width = T) 
```

- All of NBER turning points dates in the first column are identified by the DFMSDF method in the third column.

- DFMSDF method is very fast in identifying NBER turning points dates. The fourth columns shows that the calls made by the DFMSDF approach are quicker than the NBER and Giusto & Piger (2017)

???

- 1st: the month that NBER turning points occur

- 2nd: the starting date of NBER business cycles phases, which is defined as the first date after NBER turning points occur

- 3rd: dates when turning points are called using DFMSDF, following the proposed simple procedure

- 4th: the number of lag days using the DFMSDF method proposed in this paper

- 5th: the number of lag days in NBER announcements made by the NBER’s Business Cycle Dating Committee

- 6th: the number of lag days using a learning vector quantization method as in Giusto & Piger (2017)

- **During the Covid-19 pandemic crisis, the NBER announced on June 8, 2020, that a new recession started in the U.S. in March 2020. The DFMSDF model identified the start of this recession on March 22, 2020, 78 days ahead of the NBER announcement.**

- **my model identifies the start of the Great Recession 256 days ahead of the NBER announcement**

---
# Baseline Results

```{r ch3-table-trough1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
options(knitr.kable.NA = '')

trough_table <- data.frame(matrix(ncol = 6, nrow = 7))
colnames(trough_table) <- c("NBER Trough Date", "First Day of Expansion", "Date Expansion Call Available - DFMSDF", "DFMSDF Lag", "Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag")

trough_table[1,1] <- c("Jul-1980")
trough_table[2,1] <- c("Nov-1982")
trough_table[3,1] <- c("Mar-1991")
trough_table[4,1] <- c("Nov-2001")
trough_table[5,1] <- c("Jun-2009")
trough_table[6,1] <- c("Average")

trough_table[1,2] <- c("8/1/1980")
trough_table[2,2] <- c("12/1/1982")
trough_table[3,2] <- c("4/1/1991")
trough_table[4,2] <- c("12/1/2001")
trough_table[5,2] <- c("7/1/2009")

trough_table[1,3] <- c("8/10/1980")
trough_table[2,3] <- c("11/28/1982")
trough_table[3,3] <- c("6/2/1991")
trough_table[4,3] <- c("8/19/2001")
trough_table[5,3] <- c("5/24/2009")
trough_table[7,3] <- c("6/14/2020")

trough_table[1,4] <- c(9)
trough_table[2,4] <- c(-3)
trough_table[3,4] <- c(62)
trough_table[4,4] <- c(-104)
trough_table[5,4] <- c(-38)
trough_table[6,4] <- c(-15)

trough_table[1,5] <- c(341)
trough_table[2,5] <- c(219)
trough_table[3,5] <- c(631)
trough_table[4,5] <- c(593)
trough_table[5,5] <- c(446)
trough_table[6,5] <- c(446)
trough_table[7,5] <- c("NA")

trough_table[1,6] <- c(127)
trough_table[2,6] <- c(136)
trough_table[3,6] <- c(443)
trough_table[4,6] <- c(308)
trough_table[5,6] <- c(157)
trough_table[6,6] <- c(235)
trough_table[7,6] <- c("NA")

trough_table %>%
  mutate_at(c("DFMSDF Lag"), function(x) {
    cell_spec(x, bold = T,
              color = spec_color(x, end = 0.9)
              )
  }) %>%
  kable(escape = F, align = "c") %>%
  kable_styling(c("striped", "condensed"), font_size = 12, full_width = T) 
```

- In some cases, the DFMSDF method identifies business cycle phases prior to their starting dates.   
  - March 2001 peak, November 1982 trough, November 2001 trough, June 2009 trough
  - This striking result shows that the DFMSDF algorithm not only nowcasts recessions and expansions in a timely manner, but might also be powerful in forecasting recessions and expansions. 

???

- **The reason for this is that in addition to coincident data, Treasury yields (TY) is included in the analysis, which is a leading data in terms of recessions.**

- **While a new expansion has not been classified by the NBER, the DFMSDF model identified the end of this recession and the beginning of a new expansion on June 14, 2020, as most of the states have reopened since May.**

---
# Baseline Results

- False Recessions and False Expansions Identified in Real-time (Threshold=0.8):

```{r ch3-table-false1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
options(knitr.kable.NA = '')

false_table <- data.frame(matrix(ncol = 4, nrow = 7))
colnames(false_table) <- c("False Recessions","Duration", "False Expansions", "Duration")

false_table[1,1] <- c("10/7/1984 - 11/11/1984")
false_table[1,2] <- c("6 weeks")

false_table[2,1] <- c("5/4/1986 - 5/18/1986")
false_table[2,2] <- c("3 weeks")

false_table[3,1] <- c("1/24/1988 - 2/7/1988")
false_table[3,2] <- c("3 weeks")

false_table[4,1] <- c("7/9/1989 - 8/6/1989")
false_table[4,2] <- c("5 weeks")

false_table[5,1] <- c("4/23/1995 - 6/18/1995")
false_table[5,2] <- c("9 weeks")

false_table[6,1] <- c("4/13/2003 - 5/4/2003")
false_table[6,2] <- c("4 weeks")

false_table[7,1] <- c("5/14/2006 - 5/21/2006")
false_table[7,2] <- c("2 weeks")

false_table[1,3] <- c("2/21/1982 - 7/25/1982")
false_table[1,4] <- c("23 weeks")

false_table[2,3] <- c("6/15/2008 - 7/6/2008")
false_table[2,4] <- c("4 weeks")

false_table[3,3] <- c("7/27/2008 - 8/17/2008")
false_table[3,4] <- c("4 weeks")

kable(false_table, booktabs = T) %>%
  kable_styling(full_width = T, font_size = 12, position = "center") 
```

- The procedure based on daily data produces several false positives and false negatives

- Most of these signals were only produced for a relatively short amount of time. As macroeconomic policy takes time to be implemented, it is unlikely that policy mistakes predicated on these signals would have been large


---
# Baseline Results

- **Robustness Check #1**: using the threshold of 0.9 
  - The results are qualitatively similar to those for the 0.8 threshold
  - significant less false expansions and false recessions are identified in this case. This is to be expected, as the higher threshold should lead to less turning points detected, and thus less false turning points.


--

- **Robustness Check #2**: additional monthly variables are incorporated into the analysis
  - Industrial production (INDPRO), a monthly variable
  - The addition of industrial production helps identify many turning points more quickly, but at the cost of generating more false positives and false negatives.



---
# A News-Based Sentiment Index of Macroeconomic Activity

- The information encoded in text has been recently used in empirical economics research as a complement to the more structured macroeconomic and financial data traditionally used
  - Gentzkow, Kelly, & Taddy (2019), Kelly, Manela, & Moreira (2019), Shapiro, Sudhof, & Wilson (2020), Bybee, Kelly, Manela, & Xiu (2020) 

--

- Text selected from news articles arrives daily and is not revised, making textual data an ideal candidate to build more accurate nowcasting models about aggregate economic activity in real time

--

- Details of construction of text-based approach to create a high-frequency News-Based Sentiment Indicator (NBSI) regarding aggregate economic conditions can be referred to my other paper

--

- In this section I evaluate the ability of this indicator to improve turning point identification in real time over the use of the index $\hat{x_t}$ studied in previous sections


---
# A News-Based Sentiment Index of Macroeconomic Activity

- To evaluate the nowcasting ability of the NBSI on the U.S. recessions from April 1991 to August 2020, I incorporate NBSI in the business cycle phase nowcasting models developed in previous sections, and evaluate the contribution of NBSI to identify U.S. recessions in real time. 

- I fit $\Delta\hat{x_t}$ and $NBSI_t$ to a bivariate version of the Markov regime-switching $AR(0)$ process with a switching mean: 

$$\begin{align}
  \begin{bmatrix}
    \Delta\hat{x_t} \\
    NBSI_t
  \end{bmatrix} &=
  \begin{bmatrix}
    \beta_{11} \\
    \beta_{12}
  \end{bmatrix} +
  \begin{bmatrix}
   \beta_{12}  \\
   \beta_{22}
  \end{bmatrix} \times
    S_t +
  \begin{bmatrix}
    \epsilon_{1t} \\
    \epsilon_{2t}
  \end{bmatrix} \\
  \begin{bmatrix}
    \epsilon_{1t} \\
    \epsilon_{2t}
  \end{bmatrix} &\sim 
    N \left( \begin{bmatrix}
    0 \\
    0
  \end{bmatrix},  
  \begin{bmatrix}
    \sigma_{11}^2 & \sigma_{12}^2\\
    \sigma_{21}^2 & \sigma_{22}^2
  \end{bmatrix} \right) \\
    \beta_1 &<0 \\
    \beta_2 &<0 \\
    \sigma_{12}^2 &= \sigma_{21}^2
  \end{align}$$


---
# A News-Based Sentiment Index of Macroeconomic Activity

- Recessions Identified in Real-time, with NBSI **Included** (Threshold=0.8): 

```{r ch3-table-short-peak1, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
options(knitr.kable.NA = '')

peak_table <- data.frame(matrix(ncol = 6, nrow = 2))
colnames(peak_table) <- c("NBER Peak Date", "First Day of Recession", "Date Recession Call Available - DFMSDF", "DFMSDF Lag", "Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag")

peak_table[1,1] <- c("Dec-2007")
peak_table[2,1] <- c("Mar-2020")

peak_table[1,2] <- c("1/1/2008")
peak_table[2,2] <- c("3/1/2020")

peak_table[1,3] <- c("12/2/2007")
peak_table[2,3] <- c("3/29/2020")

peak_table[1,4] <- c("-30")
peak_table[2,4] <- c("28")

peak_table[1,5] <- c("335")
peak_table[2,5] <- c("99")

peak_table[1,6] <- c("158")
peak_table[2,6] <- c("NA")

kable(peak_table, booktabs = T) %>%
  kable_styling(full_width = F, font_size = 12, position = "center")
```

- Recessions Identified in Real-time, with NBSI **Excluded** (Threshold=0.8):

```{r ch3-table-short-peak2, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
options(knitr.kable.NA = '')

peak_table <- data.frame(matrix(ncol = 6, nrow = 2))
colnames(peak_table) <- c("NBER Peak Date", "First Day of Recession", "Date Recession Call Available - DFMSDF", "DFMSDF Lag", "Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag")

peak_table[1,1] <- c("Dec-2007")
peak_table[2,1] <- c("Mar-2020")

peak_table[1,2] <- c("1/1/2008")
peak_table[2,2] <- c("3/1/2020")

peak_table[1,3] <- c("4/6/2008")
peak_table[2,3] <- c("3/29/2020")

peak_table[1,4] <- c("96")
peak_table[2,4] <- c("28")

peak_table[1,5] <- c("335")
peak_table[2,5] <- c("99")

peak_table[1,6] <- c("158")
peak_table[2,6] <- c("NA")

kable(peak_table, booktabs = T) %>%
  kable_styling(full_width = F, font_size = 12, position = "center")
```

- Excluding NBSI slowers the identification of the June 2009 Peak

---
# A News-Based Sentiment Index of Macroeconomic Activity

- Expansions Identified in Real-time, with NBSI **Included** (Threshold=0.8): 

```{r ch3-table-short-trough1, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
options(knitr.kable.NA = '')

trough_table <- data.frame(matrix(ncol = 6, nrow = 2))
colnames(trough_table) <- c("NBER Trough Date", "First Day of Expansion", "Date Expansion Call Available - DFMSDF", "DFMSDF Lag", "Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag")

trough_table[1,1] <- c("Jun-2009")

trough_table[1,2] <- c("7/1/2009")

trough_table[1,3] <- c("6/7/2009")
trough_table[2,3] <- c("6/21/2020")

trough_table[1,4] <- c("-24")

trough_table[1,5] <- c("446")

trough_table[1,6] <- c("235")

kable(trough_table,  booktabs = T) %>%
  kable_styling(full_width = F, font_size = 12, position = "center")
```

- Expansions Identified in Real-time, with NBSI **Excluded** (Threshold=0.8): 

```{r ch3-table-short-trough2, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
options(knitr.kable.NA = '')

trough_table <- data.frame(matrix(ncol = 6, nrow = 2))
colnames(trough_table) <- c("NBER Trough Date", "First Day of Expansion", "Date Expansion Call Available - DFMSDF", "DFMSDF Lag", "Business Cycle Dating Committee Lag", "Giusto & Piger (2017) Lag")

trough_table[1,1] <- c("Jun-2009")

trough_table[1,2] <- c("7/1/2009")

trough_table[1,3] <- c("5/31/2009")
trough_table[2,3] <- c("6/21/2020")

trough_table[1,4] <- c("-31")

trough_table[1,5] <- c("446")

trough_table[1,6] <- c("235")

kable(trough_table, booktabs = T) %>%
  kable_styling(full_width = F, font_size = 12, position = "center") 
```


---
# A News-Based Sentiment Index of Macroeconomic Activity

- False Identifications in Real-time, with NBSI **Included** (Threshold=0.8): 

```{r ch3-table-short-false1, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
options(knitr.kable.NA = '')

false_table <- data.frame(matrix(ncol = 4, nrow = 3))
colnames(false_table) <- c("False Recessions","Duration", "False Expansions", "Duration")

false_table[1,1] <- c("7/10/2011 - 11/13/2011")
false_table[1,2] <- c("19 weeks")

false_table[2,1] <- c("1/10/2016 - 2/7/2016")
false_table[2,2] <- c("5 weeks")

false_table[2,1] <- c("12/2/2018 - 12/9/2018")
false_table[2,2] <- c("2 weeks")

kable(false_table, booktabs = T, escape = F) %>%
  kable_styling(full_width = F, font_size = 12, position = "center")
```

- False Identifications in Real-time, with NBSI **Excluded** (Threshold=0.8): 

```{r ch3-table-short-false2, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
options(knitr.kable.NA = '')

false_table <- data.frame(matrix(ncol = 4, nrow = 3))
colnames(false_table) <- c("False Recessions","Duration", "False Expansions", "Duration")

false_table[1,1] <- c("5/21/2006 - 5/28/2006")
false_table[1,2] <- c("2 weeks")

false_table[2,1] <- c("1/24/2016")
false_table[2,2] <- c("1 week")

false_table[3,1] <- c("12/9/2018")
false_table[3,2] <- c("1 week")

false_table[1,3] <- c("6/8/2008")
false_table[1,4] <- c("1 week")

false_table[2,3] <- c("6/22/2008 - 7/13/2008")
false_table[2,4] <- c("4 weeks")

false_table[3,3] <- c("8/3/2008 - 8/24/2008")
false_table[3,4] <- c("4 weeks")

kable(false_table, booktabs = T, escape = F) %>%
  kable_styling(full_width = F, font_size = 12, position = "center")
```

- Excluding NBSI produces more false identifications. 

- The contribution of the news-based sentiment index is promising but more data is needed before concluding that the index helps to identify recessions in real time over the use of the coincident index $\hat{x_t}$ alone


---
# Conclusions

- This paper is the first to systematically evaluate the ability of high frequency data to improve upon the timeliness with which new expansions and recessions can be identified. 

- I take a three-step approach and find that the use of higher frequency data significantly and consistently improves the speed at which expansions and recessions can be identified in the United States since 1980. 

  - As representative examples, my model identifies the start of the Great Recession 256 days ahead of the NBER announcement. 
  - During the Covid-19 pandemic, the NBER announced on June 8, 2020 that a new recession started in the U.S. in March 2020, while my model identified the start of this recession on March 22, 2020, 78 days ahead of the NBER announcement.




---
exclude: true

```{r, generate pdfs, include = F, eval = F}
# method 1:
pagedown::chrome_print("Prospectus_2020May29.html", output = "Prospectus_2020May29.pdf")
pagedown::chrome_print("Prospectus_2020May29.html", output = "Prospectus_2020May29.pdf")
# can set longer time to allow for the longer conversion of the pdf document: say, 60 sec rather than 30 sec  
# need to set the working directory to be the folder that contains .html 

# method 2:
library(webshot)
file_name <- paste0("file://", normalizePath("Prospectus_2020May29.html"))
webshot(file_name, "Prospectus_2020May29.pdf")

# to maintain the incremental bullets in the pdf: use either method 1 or method 2, but need to comment out the css block on the top
```

