---
title: ""
author: ""
date: ""
output:
  bookdown::pdf_document2:
    toc: false
    toc_depth: 2
    fig_caption: yes
    number_sections: yes
bibliography: 
  - Ch4_library.bib
csl: academy-of-management-review.csl
urlcolor: black
linkcolor: black
fontsize: 10pt
linestretch: 2
geometry: "left=1.25in, right=1.25in, top=1in, bottom=1in"
indent: true
header-includes:
- \usepackage{indentfirst}
---


\begin{centering}

\vspace{2 cm}

\Huge

{\bf A New High Frequency, News Based, Indicator of Macroeconomic Activity}

\vspace{2 cm}

\Large
by

Xiang LI

(Preliminary and Incomplete)

\vspace{2 cm}

\normalsize

\end{centering}



<!-- **Abstract**:  -->

\newpage

# Introduction

The information encoded in text has been recently used in empirical economics research as a complement to the more structured macroeconomic and financial data traditionally used (@Gentzkow_Kelly_Taddy_2019). Text selected from news, social media, reports and speeches contains "soft" information missing in the quantifiable variables. Unlike most of the headline macroeconomic data that are published at a relatively low frequency and for which past observation periods are revised as more accurate estimates become available, text such as news articles arrives daily and is not revised. These advantages make data extracted from text an ideal candidate to build more accurate nowcasting models about aggregate economic activity in real time. 

In this paper I propose a text-based approach to create a high-frequency news-based sentiment indicator (NBSI) regarding aggregate economic conditions. Following dictionary methods in the Natural Language Processing (NLP) literature, NBSI is established from lead paragraphs of news articles that are related to economic activity. The corpus consists of 410,601 economic and financial news articles published at a daily frequency in the Wall Street Journal from January 1991 to March 2020. 

To evaluate the predictive ability of the NBSI, this paper provides two applications. In the first application, I use NBSI to track a wide range of economic activity measures, such as consumer confidence index, total nonfarm payroll, unemployment rate, industrial production, manufacturing and industries trade sales, and initial claims to unemployment insurance benefits. In the second application, I incorporate NBSI in the business cycle phases nowcasting models developed in Chapter 3, and estimate the contribution of NBSI to identify U.S. recessions in real time. 

This paper is organized as follows. In section 2 I review related literature. In section 3 I present a text-based approach to create a high-frequency news-based sentiment indicator (NBSI) regarding aggregate economic conditions. Section 4 presents preliminary results. Section 5 lays out a future plan for this study. 

# Literature Review 

@Choi_Varian_2012 use Google Trends search engine data, which is a real-time daily and weekly index of the volume of queries that users enter into Google. The authors use Google Trend data from January 2004 to July 2011 to forecast economic indicators including initial claims for unemployment insurance and consumer confidence, and find that simple seasonal auto-regression models that include relevant Google Trends variables tend to outperform models that exclude these predictors. @Choi_Varian_2012 concludes that Google Trends may help in predicting the present.

@Kelly_Manela_Moreira_2019 model the inclusion and repetition of words of some attributes at a monthly frequency in the Wall Street Journal front page news articles. The authors find out that the text from January 1990 to December 2010 documents contains additional information that is useful for forecasting monthly macroeconomic indicators beyond that of the benchmark dynamic factor model model. 

@Shapiro_Sudhof_Wilson_2020 derive a daily and monthly time-series sentiment from economic and financial newspaper articles from January 1980 to April 2015 with dictionary methods. The authors demonstrate that news sentiment is predictive of consumer sentiment measured using surveys. The authors also find that positive sentiment shocks increase consumption, output, and interest rates and dampen inflation.

@Bybee_Kelly_Manela_Xiu_2020 use topic models to estimate a daily time-series news attention estimates to each theme from full text content of Wall Street Journal articles from January 1984 to June 2017. The authors show that the topic-based estimates accurately track a broad range of of economic variables such as aggregate output (industrial production) and employment (non-farm payrolls). The authors also find that the estimate have incremental forecasting power above and beyond standard numerical predictors. 

This paper makes contributions by demonstrating sentiment analysis that can be used in economics research to extract textual data from nontraditional data sources. The news-based sentiment index should pick up wider range of information regarding economic activity compared with the survey-based consumer sentiment index. Therefore NBSI that arrives at a daily frequency should be a candidate as supplement to macroeconomic indicators that is available at lower frequencies in nowcasting macroeconomic activity. The second contribution of this paper is to explore the power of NBSI in nowcasting a wide variety of macroeconomic variables as well as for high-frequency monitoring of business cycle phases. 

# Methodology

From the Factiva Database, I collect a large sample of lead paragraphs of 410,601 articles published in The Wall Street Journal and The Wall Street Journal Online in the United States from April 2, 1991 to April 30, 2020 that have the following subjects: 

- News about commodity, debt, bond, equity, money and currency markets.
- Analysts' comments or recommendations about corporates and industries.
- Economic performance or indicators, government finance, monetary policy, trade or external payments.

The document downloaded from the database is a Rich Text Format (RTF) file, which contains indexes for custom search fields, such as indexes for column name (CLM), section name (SE), headline (HD), author's name (BY), word count, publication date (PD) and time (ED), source name (SN), lead paragraph (LP), langauge code (LA), subject code (NS), region code (RE), publisher name (PUB), accession number (AN). For the purpose of this topic, I only extract lead paragraphs and publication dates by detecting the presence of the pattern of the index in each piece of news articles. Then I match the lead paragraph with the publication date such that the text is represented at each point in time. Table \@ref(tab:ch4-table-example) is an example of this sample. 

```{r ch4-table-example, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
library(dplyr)
library(kableExtra)

dt <- data.frame(matrix(ncol = 2, nrow = 3))
colnames(dt)<-c("Date", "Lead Paragraph")

dt[,1] <- c("03-28-2006", "03-29-2006", "03-29-2006")

dt[1,2] <- c("Jon Corzine, New Jersey's new Governor, isn't the first politician not to follow through on a campaign promise. But rarely is such dishonesty later presented as a virtue. The question for voters to contemplate is whether this is also an indication of what to expect if Democrats gain control of Congress in November.Mr. Corzine won the Trenton statehouse last year by running as a tax cutter who'd raise property tax rebates by 40% over four years. \"I'm not considering raising taxes. It's not on my agenda. We have a very high-rate tax structure. I'm not considering it,\" the then-U.S. Senator had vowed in October.")
dt[2,2] <- c("A growing number of homeowners, riding the crest of the real-estate boom, are getting hit by an unpleasant surprise when they sell: a hefty tax bill.This development stems from a 1997 law that Treasury Department officials said at the time would eliminate capital-gains taxes for nearly everyone selling their primary residence. Under that law, most married couples who file jointly can exclude as much as $500,000 of their gain. For most singles, the limit is $250,000.")
dt[3,2] <- c("The airline most often viewed as the strongest, healthiest and best-run of the pack may be one of the weaker bets for investors hoping to profit from a budding turnaround in the industry.A strengthening market has sent some beleaguered airline stocks soaring in recent months, but shares in industry stalwart Southwest Airlines have been more sluggish. In the past year, the stock of American Airlines parent AMR Corp. has risen 168% on the New York Stock Exchange. Rival hub-and-spoke carrier Continental Airlines saw its stock price jump 142% on the Big Board during the 12-month period. But the same tide of good news has lifted Southwest's stock just 25% on the NYSE, though that gain still handily outpaced the Dow Jones Industrial Average, which was up 7% over the same period.")

kable(dt, "latex", booktabs = T, caption = "Text Example") %>%
  kable_styling(full_width = F, font_size = 9, position = "center") %>%
  column_spec(1, bold = F, border_right = F) %>%
  column_spec(2, width = "40em",border_right = F)
```

Before applying statistical methods, the raw text is extracted as a manageable high-dimensional numerical array using process criteria in the following order: 

- Exclude lead paragraphs with less than 50 characters.
- Tokenize the text into individual words and transform it to a tidy data structure.
- Strip stopwords such as ‘a’, ‘the’, ‘to’, ‘for’ out of token lists.
- Replace words with their root such that "economic", "economics", and "economically" are all replaced by the stem "economic".
- Assign "positive" or "negative" sentiment to words based on a general-purpose "Bing" lexicon from [Bing Liu and collaborators](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), which categorizes words in a binary fashion.
- Reverse sentiment of words preceded by negation words such as “no”, “not”, “never”, “without”.
- Measure sentiment scores for each lead paragraph with $\frac{n_{pos}-n_{neg}}{n_{words}}$, where $n_{pos}$, $n_{neg}$, and $n_{words}$ represent separately the number of positive, negative, and total words in each lead paragraph.
- Average over articles published on the same day to obtain the daily sentiment index.
- Average over daily sentiment to obtain the weekly sentiment index.
- Decompose the weekly sentiment into trend, seasonality and remainder with a filtering procedure developed by @Cleveland_Cleveland_McRae_Tarpenning_1990, and remove seasonality from weekly series. 

# Preliminary Results

The weekly NBSI from April 02, 1991 (the fourteenth week of 1991) to April 30, 2020 (the eighteenth week of 2020 ) is shown in Figure \@ref(fig:ch4-fig-weekly-NBSI). Shaded areas indicate U.S. recessions. The index drops sharply before the start of the two recessions in the sample period. This suggests that the index might be a leading indicator with respect to recessions and might be used to nowcast or even forecast recessions. 

```{r ch4-fig-weekly-NBSI, echo=FALSE, fig.cap='Weekly News-Based Sentiment Index (April 1991- April 2020)', fig.align='center', fig.width=8.5, out.width = "100%"}
knitr::include_graphics(c("/Users/lixiang/Desktop/Mixed-frequency/Prospectus/Figs/Ch4/NBSI_91Apr_20Apr.png"))
```

Figure \@ref(fig:ch4-fig-weekly-NBSI-2020) shows the standardized weekly NBSI for January through April in 2020. It presents how NBSI picked up the bad economic outcomes in March 2020. It is also interesting that NBSI falls in February, prior to the real problems starting in the United States.

```{r ch4-fig-weekly-NBSI-2020, echo=FALSE, fig.cap='Weekly News-Based Sentiment Index in 2020', fig.align='center', fig.width=8.5, out.width = "100%"}
knitr::include_graphics(c("/Users/lixiang/Desktop/Mixed-frequency/Prospectus/Figs/Ch4/NBSI_20Jan_20Apr.png"))
```

Next I study the correlation between the weekly NBSI and the weekly initial claims to unemployment insurance (ICSA). Due to the economic impact of Covid-19, ICSA has skyrocketed since late March 2020. Including these values makes the movement of NBSI and ICSA prior to late March less legible; therefore, Figure \@ref(fig:ch4-fig-ICSA) only shows fluctuations of the weekly NBSI and ICSA as of March 14, 2020. How NBSI tracks ICSA contemporaneously is shown in the left panel and how 4-week moving average of NBSI tracks 4-week moving average of ICSA is shown in the right panel. 

```{r ch4-fig-ICSA, echo=FALSE, fig.cap='Weekly News-Based Sentiment Index and Initial Claims to Unemployment Insurance', fig.align='center', fig.width=8.5, out.width = "100%"}
knitr::include_graphics(c("/Users/lixiang/Desktop/Mixed-frequency/Prospectus/Figs/Ch4/two_weekly.png"))
```

Table \@ref(tab:ch4-table-corr2) shows Pearson correlations between weekly variables starting from April 02, 1991 (the fourteenth week of 1991) to April 30, 2020 (the eighteenth week of 2020). P-values are in parenthesis. There is sufficient evidence to conclude that there is a significant negative linear relationship between the weekly NBSI and contemporaneous ICSA at a significance level of 0.01. There is a significant negative linear relationship between NBSI lagged 1-week and ICSA at a significance level of 0.01. There is a significant negative linear relationship between 4-week moving average of NBSI and 4-week moving average of ICSA at a significance level of 0.1. There is a significant negative linear relationship between 4-week moving average of NBSI lagged 1-week and 4-week moving average of ICSA at a significance level of 0.1. All Pearson correlations have correct signs and fairly large magnitude. 

```{r ch4-table-corr2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
options(knitr.kable.NA = '')

corr_table2 <- data.frame(matrix(ncol = 4, nrow = 2))
colnames(corr_table2) <- c("NBSI", "Moving Average of NBSI", "Lagged NBSI", "Moving Average of Lagged NBSI")
rownames(corr_table2) <- c("ICSA", "Moving Average of ICSA")

corr_table2[1,1] <- c("-0.099(<.001)")
corr_table2[2,2] <- c("-0.097(.060)")
corr_table2[1,3] <- c("-0.101(<.001)")
corr_table2[2,4] <- c("-0.099(.055)")

kable(corr_table2, "latex", booktabs = T, caption = "Correlation between Weekly NBSI and ICSA") %>%
  kable_styling(full_width = F, font_size = 9, position = "center") %>%
  column_spec(5, width = "7em") %>%
  column_spec(2, width = "7em") %>%
  column_spec(3, width = "7em") %>%
  column_spec(4, width = "7em") 

# corr_table3 <- data.frame(matrix(ncol = 4, nrow = 2))
# colnames(corr_table3) <- c("NBSI", "Moving Average of NBSI", "Lagged NBSI", "Moving Average of Lagged NBSI")
# rownames(corr_table3) <- c("ICSA", "Moving Average of ICSA")
# 
# corr_table3[1,1] <- c(9.393234e-08)
# corr_table3[2,2] <- c(0.005820867)
# corr_table3[1,3] <- c(1.884467e-08)
# corr_table3[2,4] <- c(0.003812009)
# 
# kable(corr_table3, "latex", booktabs = T, caption = "P-value of Correlation Tests between Weekly NBSI and ICSA") %>%
#   kable_styling(full_width = F, font_size = 9, position = "center") %>%
#   column_spec(5, width = "7em") %>%
#   column_spec(2, width = "7em") %>%
#   column_spec(3, width = "7em") %>%
#   column_spec(4, width = "7em") 
```

Next, I analyze the ability of the monthly NBSI to track fluctuations in the economic activity. Monthly macroeconomic variables I use include unemployment rate, total nonfarm payroll, industrial production, real manufacturing and industries sales, and University of Michigan consumer sentiment. Figure \@ref(fig:ch4-fig-monthly-NBSI) shows the movement between the monthly NBSI and monthly macroeconomic variables from April 1991 to April 2020. Contemporaneous Pearson correlations and p-values between the monthly NBSI and macroeconomic variables are shown in Table \@ref(tab:ch4-table-corr1). 

There is sufficient evidence to conclude that there is a significant negative linear relationship between the monthly NBSI and unemployment rate at a significance level of 0.05. There is also a significant negative linear relationship between the monthly NBSI and the change of unemployment rate at a significance level of 0.01. There are positive linear relationships between the monthly NBSI and all other macroeconomic variables at a significance level of 0.01. All Pearson correlations have correct signs and fairly large magnitude. 

```{r ch4-fig-monthly-NBSI, echo=FALSE, fig.cap='Monthly News-Based Sentiment Index and Macroeconomic Variables', fig.align='center', fig.width=8.5, out.width = "100%"}
knitr::include_graphics(c("/Users/lixiang/Desktop/Mixed-frequency/Prospectus/Figs/Ch4/all_monthly.png"))
```

```{r ch4-table-corr1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
corr_table <- data.frame(matrix(ncol = 2, nrow = 6))

colnames(corr_table) <- c("Pearson Correlation", "P-value")
rownames(corr_table) <- c("Unemployment Rate", "Change in Unemployment Rate","Payroll Employment Growth", "Industrial Production Growth", "Real Manufacturing and Trade Sales Growth", "University of Michigan Consumer Sentiment")

corr_table[1,1] <- c(-0.1347044)
corr_table[2,1] <- c(-0.2390528)
corr_table[3,1] <- c(0.4907129)
corr_table[4,1] <- c(0.3747335)
corr_table[5,1] <- c(0.2464932)
corr_table[6,1] <- c(0.4682164)

corr_table[1,2] <- c(.0118927)
corr_table[2,2] <- c(.0000065)
corr_table[3,2] <- c(.0000065)
corr_table[4,2] <- c(.0000065)
corr_table[5,2] <- c(.0000065)
corr_table[6,2] <- c(.0000065)

kable(corr_table, "latex", booktabs = T, caption = "Contemporaneous Correlation between Monthly NBSI and Macroeconomic Variables") %>%
  kable_styling(full_width = F, font_size = 9, position = "center") 
```

# Future Directions

There are shortcomings of dictionary methods. Words such as "federal funds rate", "increase", and "decrease" are not categorized, and words such as "unemployment" and "tax" are categorized as "negative". In cases such as "the federal funds rate increase", the dictionary method fails to contribute to the negative score; and in cases such as "the unemployment rate decreases", the dictionary method wrongly contribute to the negative score. Sentiment index constructed from dictionary methods is the average of valence scores of each word within an article. It fails to account for the context of a word, and could wrongly decipher how a word was used within a sentence. 

In order to account for the textual characteristics of words, "off-the-shelf” supervised machine learning techniques that are highly efficient in understanding the context and relation between words could be a potential solution. To achieve this, I plan to manually rate the sentiment of a fraction of news articles, which is taken as the "true" sentiment and used as training set. Then I plan to use Bidirectional Encoder Representations from Transformers (BERT), a more recently developed learning model developed at Google, to model context and sequential information in text.

<!---
to represent words in a numerical vector space. Particularly, similar words are placed close together in the vector space while dissimilar words are placed wide apart. Third, classifiers such as Logistic Regression will be trained with a scoring metric measuring accuracy. Then the sentiment of rest of the news will be evaluated by those trained classifiers.--->

For this paper, I plan to use the news-based sentiment index to the following applications: 

- Exploring nowcasting power of NBSI for monthly macroeconomic variables used in the models developed in Chapter 3 and updating the nowcast as the month evolves 

- Exploring nowcasting power for business cycle phases, within the framework proposed in Chapter 3


\newpage
## Reference